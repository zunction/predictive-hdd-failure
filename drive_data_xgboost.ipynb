{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting drive failure with XGBoost and RAPIDS\n",
    "\n",
    "**Dataset**: Hard disk SMART data and failure dataset from Backblaze ([More information](https://www.backblaze.com/b2/hard-drive-test-data.html))\n",
    "\n",
    "**Task**: Predict hard disk failure with RAPIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:36:50.489370Z",
     "start_time": "2018-12-23T14:36:49.322494Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn.metrics import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# RAPIDS\n",
    "import cudf\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Data\n",
    "\n",
    "#### Training Data\n",
    "\n",
    "Use Pandas to load training data from CSV ([download link](https://s3-ap-southeast-1.amazonaws.com/deeplearning-iap-material/hdd_test_data/train.csv)). This consists of the pre-processed drive data from **January 2015** to **September 2018**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://s3-ap-southeast-1.amazonaws.com/deeplearning-mat/hdd_test_data/train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:37:13.856311Z",
     "start_time": "2018-12-23T14:36:50.491244Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into features (`df_train`) and labels(`df_target`), where each is a Pandas `Dataframe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:37:16.597909Z",
     "start_time": "2018-12-23T14:37:13.857591Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = df.drop([\"failure\"],axis=1).apply(pd.to_numeric).astype(np.float32)\n",
    "df_train_target = pd.DataFrame(df[\"failure\"]).apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Data\n",
    "\n",
    "Do the same thing to load evaluation data from CSV ([download link](https://s3-ap-southeast-1.amazonaws.com/deeplearning-iap-material/hdd_test_data/eval.csv)), if you have a seperate file to load.\n",
    "\n",
    "In our case, this consists of the pre-processed drive data from **October 2017 to December 2018**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://s3-ap-southeast-1.amazonaws.com/deeplearning-mat/hdd_test_data/eval.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:37:29.019045Z",
     "start_time": "2018-12-23T14:37:16.599916Z"
    }
   },
   "outputs": [],
   "source": [
    "df_t = pd.read_csv(\"eval.csv\")\n",
    "df_t.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_t.drop([\"failure\"],axis=1).apply(pd.to_numeric).astype(np.float32)\n",
    "df_test_target = pd.DataFrame(df_t[\"failure\"]).apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TREE_DEPTH = 8\n",
    "TREE_METHOD = 'hist'\n",
    "ITERATIONS = 85\n",
    "SUBSAMPLE = 0.6\n",
    "REGULARIZATION = 1.3\n",
    "GAMMA = 0.3\n",
    "POS_WEIGHT = 1\n",
    "EARLY_STOP = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train with CPU\n",
    "\n",
    "XGBoost training with CPU (`params[tree_method] = 'hist'`), using a Pandas `Dataframe` loaded into `xgb.DMatrix`. For more information, check out [this page in the XGBoost Documentation](https://xgboost.readthedocs.io/en/latest/python/python_intro.html).\n",
    "\n",
    "As we can see, training with even a high-end Intel Xeon CPU is pretty slow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu | grep 'Model name:'\n",
    "!lscpu | grep 'CPU(s)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "xgtrain = xgb.DMatrix(df_train, df_train_target)\n",
    "xgeval = xgb.DMatrix(df_test, df_test_target)\n",
    "\n",
    "params = {'tree_method': TREE_METHOD, 'max_depth': MAX_TREE_DEPTH, 'alpha': REGULARIZATION,\n",
    "          'gamma': GAMMA, 'subsample': SUBSAMPLE, 'scale_pos_weight': POS_WEIGHT, 'learning_rate': 0.05, 'silent': 1}\n",
    "\n",
    "bst = xgb.train(params, xgtrain, ITERATIONS, evals=[(xgtrain, \"train\"), (xgeval, \"eval\")],\n",
    "                early_stopping_rounds=EARLY_STOP)\n",
    "\n",
    "timetaken_cpu = time.time() - start_time\n",
    "\n",
    "# free up memory\n",
    "del xgtrain\n",
    "del bst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train with GPU\n",
    "\n",
    "To use GPU, we set `params[tree_method] = 'gpu_hist'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:38:13.023281Z",
     "start_time": "2018-12-23T14:37:29.020880Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPU, without using cuDF\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "xgtrain = xgb.DMatrix(df_train, df_train_target)\n",
    "xgeval = xgb.DMatrix(df_test, df_test_target)\n",
    "\n",
    "params = {'tree_method': \"gpu_\"+TREE_METHOD, 'max_depth': MAX_TREE_DEPTH, 'alpha': REGULARIZATION,\n",
    "          'gamma': GAMMA, 'subsample': SUBSAMPLE, 'scale_pos_weight': POS_WEIGHT, 'learning_rate': 0.05, 'silent': 1}\n",
    "\n",
    "bst = xgb.train(params, xgtrain, ITERATIONS, evals=[(xgtrain, \"train\"), (xgeval, \"eval\")],\n",
    "                early_stopping_rounds=EARLY_STOP)\n",
    "\n",
    "timetaken_gpu_nocudf = time.time() - start_time\n",
    "\n",
    "# free up memory\n",
    "del xgtrain\n",
    "del bst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use full RAPIDS stack by using XGBoost with cuDF for additional speedup. To do this, we load the Pandas `Dataframe` into a cuDF Dataframe (Python object type `cudf.dataframe.dataframe.DataFrame`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:38:14.073981Z",
     "start_time": "2018-12-23T14:38:13.024721Z"
    }
   },
   "outputs": [],
   "source": [
    "# load into cuDF Dataframe\n",
    "\n",
    "gdf_train = cudf.DataFrame.from_pandas(df_train)\n",
    "gdf_train_target = cudf.DataFrame.from_pandas(df_train_target)\n",
    "\n",
    "gdf_eval = cudf.DataFrame.from_pandas(df_test)\n",
    "gdf_eval_target = cudf.DataFrame.from_pandas(df_test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:38:28.652178Z",
     "start_time": "2018-12-23T14:38:14.075351Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPU, with using cuDF\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "xgtrain = xgb.DMatrix(gdf_train, gdf_train_target)\n",
    "xgeval = xgb.DMatrix(gdf_eval, gdf_eval_target)\n",
    "\n",
    "params = {'tree_method': \"gpu_\"+TREE_METHOD, 'max_depth': MAX_TREE_DEPTH, 'alpha': REGULARIZATION,\n",
    "          'gamma': GAMMA, 'subsample': SUBSAMPLE, 'scale_pos_weight': POS_WEIGHT, 'learning_rate': 0.05, 'silent': 1}\n",
    "\n",
    "bst = xgb.train(params, xgtrain, ITERATIONS, evals=[(xgtrain, \"train\"), (xgeval, \"eval\")],\n",
    "                early_stopping_rounds=EARLY_STOP)\n",
    "\n",
    "timetaken_gpu = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:38:29.532366Z",
     "start_time": "2018-12-23T14:38:28.653601Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Check GPU memory usage\")\n",
    "!gpustat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Results\n",
    "\n",
    "We see a significant speed-up when we use the RAPIDS stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-23T14:38:29.537586Z",
     "start_time": "2018-12-23T14:38:29.534001Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"CPU Time Taken:\\n\", round(timetaken_cpu,1))\n",
    "print(\"\\nGPU (no cuDF) Time Taken:\\n\", round(timetaken_gpu_nocudf,1))\n",
    "print(\"\\nGPU (cuDF) Time Taken:\\n\", round(timetaken_gpu,1))\n",
    "print(\"\\nTotal speed-up with RAPIDS:\\n\", round(timetaken_cpu/timetaken_gpu*100,1), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the model's performance on the evalutation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = bst.predict(xgeval)\n",
    "\n",
    "y_pred = []\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "for pred in preds:\n",
    "    if pred<=THRESHOLD:\n",
    "        y_pred.append(0)\n",
    "    if pred>THRESHOLD:\n",
    "        y_pred.append(1)\n",
    "\n",
    "y_pred = np.asarray(y_pred)\n",
    "        \n",
    "y_true = df_test_target.values.reshape(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy (Eval)\", round(accuracy_score(y_true, y_pred),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=[\"normal\", \"fail\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-dark')\n",
    "def plot_confusion_matrix(cm, labels,\n",
    "                          normalize=True,\n",
    "                          title='Confusion Matrix (Validation Set)',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "plt.figure(figsize=(14,7))\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion_matrix(cnf_matrix, labels=[\"normal\", \"fail\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
